{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf### models\n",
    "import numpy as np### math computations\n",
    "import matplotlib.pyplot as plt### plotting bar chart\n",
    "import sklearn### machine learning library\n",
    "import cv2## image processing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
    "import seaborn as sns### visualizations\n",
    "import datetime\n",
    "import pathlib\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from numpy import random\n",
    "import gensim.downloader as api\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense,Flatten,InputLayer,BatchNormalization,Dropout,Input,LayerNormalization\n",
    "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import (DataCollatorWithPadding,create_optimizer,DebertaTokenizerFast)\n",
    "from transformers import LongformerTokenizerFast,TFLongformerForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"covid_qa_deepset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"allenai/longformer-large-4096-finetuned-triviaqa\"\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples = tokenizer(\n",
    "    dataset['train'][0]['question'],\n",
    "    dataset['train'][0]['context'],\n",
    "    truncation = 'only_second',\n",
    "    max_length = MAX_LENGTH,\n",
    "    stride = 64,\n",
    "    return_overflowing_tokens = True,\n",
    "    return_offsets_mapping = True,\n",
    "    padding = \"max_length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_examples['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_token=tokenizer.tokenize(\"[CLS]What is the main cause of HIV-1 infection in children?[SEP]Functional Genetic Variants in DC-SIGNR Are Associated with Mother-to-Child Transmission of HIV-1\\n\\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2752805/\\n\\nBoily-Larouche, Geneviève; Iscache, Anne-Laure; Zijenah, Lynn S.; Humphrey, Jean H.; Mouland, Andrew J.; Ward, Brian J.; Roger, Michel\\n2009-10-07\\nDOI:10.1371/journal.pone.0007211\\nLicense:cc-by\\n\\nAbstract: BACKGROUND: Mother-to-child transmission (MTCT) is the main cause of HIV-1 infection in children worldwide.\")\n",
    "print(list_token)\n",
    "for i in range(len(list_token)):\n",
    "    if list_token[i]=='Ġchildren':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"What is the main cause of HIV-1 infection in children?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in tokenized_examples[\"input_ids\"]:\n",
    "    print(ids)\n",
    "    print('-->',tokenizer.decode(ids))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mapping_list=[(0, 4), (4, 7), (7, 11), (11, 16), (16, 22), (22, 25), (25, 29), (29, 30), (30, 31), (31, 41), (41, 44), (44, 53), (53, 54), (0, 0), (0, 8), (8, 10), (10, 18), (18, 23), (23, 27), (27, 30), (30, 33), (33, 34), (34, 38), (38, 39), (39, 43), (43, 54), (54, 59), (59, 66), (66, 67), (67, 69), (69, 70), (70, 75), (75, 88), (88, 91), (91, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 104), (104, 107), (107, 110), (110, 111), (111, 113), (113, 115), (115, 116), (116, 118), (118, 119), (119, 120), (120, 123), (123, 124), (124, 127), (127, 128), (128, 130), (130, 131), (131, 132), (132, 140), (140, 141), (141, 143), (143, 144), (144, 146), (146, 148), (148, 151), (151, 152), (152, 153), (153, 154), (154, 156), (156, 159), (159, 160), (160, 161), (161, 163), (163, 165), (165, 168), (168, 169), (169, 174), (174, 176), (176, 177), (177, 179), (179, 180), (180, 183), (183, 188), (188, 189), (189, 194), (194, 195), (195, 197), (197, 200), (200, 201), (201, 203), (203, 205), (205, 207), (207, 209), (209, 210), (210, 215), (215, 217), (217, 219), (219, 225), (225, 228), (228, 229), (229, 234), (234, 236), (236, 238), (238, 242), (242, 246), (246, 247), (247, 254), (254, 256), (256, 258), (258, 263), (263, 264), (264, 270), (270, 272), (272, 274), (274, 280), (280, 281), (281, 288), (288, 289), (289, 293), (293, 294), (294, 296), (296, 297), (297, 299), (299, 300), (300, 302), (302, 303), (303, 304), (304, 306), (306, 307), (307, 309), (309, 311), (311, 312), (312, 319), (319, 320), (320, 321), (321, 324), (324, 325), (325, 328), (328, 330), (330, 332), (332, 333), (333, 340), (340, 341), (341, 343), (343, 344), (344, 346), (346, 347), (347, 348), (348, 356), (356, 357), (357, 362), (362, 368), (368, 369), (369, 376),]\n",
    "print(len(offset_mapping_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n",
    "offset_mapping = tokenized_examples.pop('offset_mapping')\n",
    "\n",
    "tokenized_examples['start_positions'] = []\n",
    "tokenized_examples['end_positions'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , offsets in enumerate(offset_mapping):\n",
    "    if(len(dataset['train'][0]['answers']['answer_start'])==0):\n",
    "        tokenized_examples['start_positions'].append(0)\n",
    "        tokenized_examples['end_positions'].append(0)\n",
    "    else:\n",
    "        start_char = dataset['train'][0]['answers']['answer_start'][0]\n",
    "        end_char =start_char + len(dataset['train'][0]['answers']['text'][0])\n",
    "        found = 0\n",
    "        start_token_position = 0\n",
    "        end_token_position = 0\n",
    "\n",
    "        for j, offset in enumerate(offsets):\n",
    "            if offset[0]<=start_char and offset[1]>=start_char and found==0:\n",
    "                start_token_position = j\n",
    "                end_token_position = MAX_LENGTH\n",
    "                found = 1\n",
    "            if offset[1]>=end_char and found==1:\n",
    "                end_token_position = j\n",
    "                break\n",
    "        tokenized_examples['start_positions'].append(start_token_position)\n",
    "        tokenized_examples['end_positions'].append(end_token_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_examples['start_positions'])\n",
    "print(tokenized_examples['end_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_examples['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "  \n",
    "  questions = [q.lstrip() for q in dataset[\"question\"]]\n",
    "  paragraphs = [p.lstrip() for p in dataset[\"context\"]]\n",
    "  \n",
    "  tokenized_examples = tokenizer(\n",
    "    questions,\n",
    "    paragraphs,\n",
    "    truncation=\"only_second\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    stride=64,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "  sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "  offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "  tokenized_examples[\"start_positions\"] = []#[152,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "  tokenized_examples[\"end_positions\"] = []#[172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "  for i, offsets in enumerate(offset_mapping):\n",
    "    sample_index = sample_mapping[i]\n",
    "\n",
    "    start_char=dataset[\"answers\"][sample_index]['answer_start'][0]\n",
    "    end_char=start_char+len(dataset[\"answers\"][sample_index]['text'][0])\n",
    "    found=0\n",
    "    start_token_position=0\n",
    "    end_token_position=0\n",
    "    \n",
    "    for j,offset in enumerate(offsets):\n",
    "      if offset[0]<=start_char and offset[1]>=start_char and found==0:\n",
    "        start_token_position=j\n",
    "        end_token_position=MAX_LENGTH\n",
    "        found=1\n",
    "      if offset[1]>=end_char and found==1:\n",
    "        end_token_position=j\n",
    "        break\n",
    "    tokenized_examples[\"start_positions\"].append(start_token_position)\n",
    "    tokenized_examples[\"end_positions\"].append(end_token_position)\n",
    "\n",
    "  return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset=dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=tf_dataset.take(int(0.9*len(tf_dataset)))\n",
    "val_dataset=tf_dataset.skip(int(0.9*len(tf_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer, TFLongformerForQuestionAnswering\n",
    "\n",
    "model = TFLongformerForQuestionAnswering.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_dataset,validation_data=val_dataset,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_metric = load(\"squad\")\n",
    "predictions = [{'prediction_text': '1999', 'id': '56e10a3be3433e1400422b22'}]\n",
    "references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"How is the virus spread?\"\n",
    "text=\"We know that the disease is caused by the SARS-CoV-2 virus, which spreads between people in several different ways.Current evidence suggests that the virus spreads mainly between people who are in close contact with each other, for example at a conversational distance.The virus can spread from an infected person’s mouth or nose in small liquid particles when they cough, sneeze, speak, sing or breathe. Another person can then contract the virus when infectious particles that pass through the air are inhaled at short range (this is often called short-range aerosol or short-range airborne transmission) or if infectious particles come into direct contact with the eyes, nose, or mouth (droplet transmission). The virus can also spread in poorly ventilated and/or crowded indoor settings, where people tend to spend longer periods of time. This is because aerosols can remain suspended in the air or travel farther than conversational distance (this is often called long-range aerosol or long-range airborne transmission). People may also become infected when touching their eyes, nose or mouth after touching surfaces or objects that have been contaminated by the virus. Further research is ongoing to better understand the spread of the virus and which settings are most risky and why. Research is also under way to study virus variants that are emerging and why some are more transmissible. For updated information on SARS-CoV-2 variants, please read the weekly epidemiologic updates.\"\n",
    "inputs = tokenizer(question, text, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
